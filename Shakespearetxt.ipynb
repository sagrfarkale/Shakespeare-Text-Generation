{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Shakespearetxt.ipynb",
      "provenance": [],
      "mount_file_id": "1-EJC0b0DW870Y4xtByeZARugA78DrYUl",
      "authorship_tag": "ABX9TyOYkVLI9jr8mVk71CEA0ZDw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagrfarkale/Shakespeare-Text-Generation/blob/main/Shakespearetxt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbzifuSUyfuv"
      },
      "source": [
        "## RNN for Text Generation through LSTM(CharacterModel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8_vUUKhNQP1"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BqS_ZkEy5Rw"
      },
      "source": [
        "Importing Torch ,Numpy ,Matplotlib Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Wwvrklkb5iS"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6elc9tOzKAo"
      },
      "source": [
        "Loading data from Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWCKlJsScHFc",
        "outputId": "a0650659-d40c-4044-9574-79e15259b26b"
      },
      "source": [
        "%cd '/content/drive/MyDrive/Pytorch/PYTORCH_NOTEBOOKS/PYTORCH_NOTEBOOKS/Data'\n",
        "with open('shakespeare.txt','r',encoding='utf8') as f:\n",
        "    text = f.read()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Pytorch/PYTORCH_NOTEBOOKS/PYTORCH_NOTEBOOKS/Data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0rqajLvcHOa",
        "outputId": "8e367f0e-9c69-445a-f638-18ee2af4bd7d"
      },
      "source": [
        "print(text[:1000])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "                     1\n",
            "  From fairest creatures we desire increase,\n",
            "  That thereby beauty's rose might never die,\n",
            "  But as the riper should by time decease,\n",
            "  His tender heir might bear his memory:\n",
            "  But thou contracted to thine own bright eyes,\n",
            "  Feed'st thy light's flame with self-substantial fuel,\n",
            "  Making a famine where abundance lies,\n",
            "  Thy self thy foe, to thy sweet self too cruel:\n",
            "  Thou that art now the world's fresh ornament,\n",
            "  And only herald to the gaudy spring,\n",
            "  Within thine own bud buriest thy content,\n",
            "  And tender churl mak'st waste in niggarding:\n",
            "    Pity the world, or else this glutton be,\n",
            "    To eat the world's due, by the grave and thee.\n",
            "\n",
            "\n",
            "                     2\n",
            "  When forty winters shall besiege thy brow,\n",
            "  And dig deep trenches in thy beauty's field,\n",
            "  Thy youth's proud livery so gazed on now,\n",
            "  Will be a tattered weed of small worth held:  \n",
            "  Then being asked, where all thy beauty lies,\n",
            "  Where all the treasure of thy lusty days;\n",
            "  To say within thine own deep su\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aR0e_VLCcHV5",
        "outputId": "35f15f98-013d-4cab-8a3e-d1baff625f64"
      },
      "source": [
        "len(text)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5445609"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2PGsv4-zZux"
      },
      "source": [
        "Total unique characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xk0cdpyScHUY"
      },
      "source": [
        "all_characters = set(text)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y55A861Rzlwg"
      },
      "source": [
        "Indexing unique character "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIiLeHf0cHYv"
      },
      "source": [
        "decoder = dict(enumerate(all_characters))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HaKCcwMscHaG"
      },
      "source": [
        "#decoder"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9JYVfzNz1C6"
      },
      "source": [
        "Dictionary to replicate respective index for Characters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZB9jAWDcHbQ"
      },
      "source": [
        "encoder = {char:idx for idx,char in decoder.items()}\n"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdC6ut9McHcl"
      },
      "source": [
        "#encoder"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tN_H5090D5n"
      },
      "source": [
        "list of array of whole text with their encoded number"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLJbM0J2cHey"
      },
      "source": [
        "encoded_text = np.array([encoder[char] for char in text])"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48L7TM6gcHhb",
        "outputId": "8f99667b-2c3b-4ab3-ae90-4d8731bc332e"
      },
      "source": [
        "encoded_text[:100]"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([16, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41,\n",
              "       41, 41, 41, 41, 41, 26, 16, 41, 41, 60, 62, 50, 70, 41, 10, 82, 20,\n",
              "       62, 33, 39, 61, 41, 49, 62, 33, 82, 61, 81, 62, 33, 39, 41, 27, 33,\n",
              "       41, 66, 33, 39, 20, 62, 33, 41, 20, 55, 49, 62, 33, 82, 39, 33,  8,\n",
              "       16, 41, 41, 15, 14, 82, 61, 41, 61, 14, 33, 62, 33, 56, 42, 41, 56,\n",
              "       33, 82, 81, 61, 42, 46, 39, 41, 62, 50, 39, 33, 41, 70, 20])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IKKy0leK0TwI"
      },
      "source": [
        "One hot Encoding characters\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfuUgzvie7gN"
      },
      "source": [
        "def one_hot_encoder(encoded_text, num_uni_chars):\n",
        "    '''\n",
        "    encoded_text : batch of encoded text\n",
        "    \n",
        "    num_uni_chars = number of unique characters (len(set(text)))\n",
        "    '''\n",
        "    \n",
        "    # METHOD FROM:\n",
        "    # https://stackoverflow.com/questions/29831489/convert-encoded_textay-of-indices-to-1-hot-encoded-numpy-encoded_textay\n",
        "      \n",
        "    # Create a placeholder for zeros.\n",
        "    one_hot = np.zeros((encoded_text.size, num_uni_chars))\n",
        "    \n",
        "    # Convert data type for later use with pytorch (errors if we dont!)\n",
        "    one_hot = one_hot.astype(np.float32)\n",
        "\n",
        "    # Using fancy indexing fill in the 1s at the correct index locations\n",
        "    one_hot[np.arange(one_hot.shape[0]), encoded_text.flatten()] = 1.0\n",
        "    \n",
        "\n",
        "    # Reshape it so it matches the batch sahe\n",
        "    one_hot = one_hot.reshape((*encoded_text.shape, num_uni_chars))\n",
        "    \n",
        "    return one_hot"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9ddbhZqfMRf",
        "outputId": "642afa15-bb6b-46c9-eb45-f7e9c55b2c19"
      },
      "source": [
        "#example\n",
        "one_hot_encoder(np.array([1,2,0]),3)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZOYP30B0zMY"
      },
      "source": [
        "Function to generate batches for training\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqLLfcJHfRrK"
      },
      "source": [
        "def generate_batches(encoded_text, samp_per_batch=10, seq_len=50):\n",
        "    \n",
        "    '''\n",
        "    Generate (using yield) batches for training.\n",
        "    \n",
        "    X: Encoded Text of length seq_len\n",
        "    Y: Encoded Text shifted by one\n",
        "    \n",
        "    Example:\n",
        "    \n",
        "    X:\n",
        "    \n",
        "    [[1 2 3]]\n",
        "    \n",
        "    Y:\n",
        "    \n",
        "    [[ 2 3 4]]\n",
        "    \n",
        "    encoded_text : Complete Encoded Text to make batches from\n",
        "    batch_size : Number of samples per batch\n",
        "    seq_len : Length of character sequence\n",
        "       \n",
        "    '''\n",
        "    \n",
        "    # Total number of characters per batch\n",
        "    # Example: If samp_per_batch is 2 and seq_len is 50, then 100\n",
        "    # characters come out per batch.\n",
        "    char_per_batch = samp_per_batch * seq_len\n",
        "    \n",
        "    \n",
        "    # Number of batches available to make\n",
        "    # Use int() to roun to nearest integer\n",
        "    num_batches_avail = int(len(encoded_text)/char_per_batch)\n",
        "    \n",
        "    # Cut off end of encoded_text that\n",
        "    # won't fit evenly into a batch\n",
        "    encoded_text = encoded_text[:num_batches_avail * char_per_batch]\n",
        "    \n",
        "    \n",
        "    # Reshape text into rows the size of a batch\n",
        "    encoded_text = encoded_text.reshape((samp_per_batch, -1))\n",
        "    \n",
        "\n",
        "    # Go through each row in array.\n",
        "    for n in range(0, encoded_text.shape[1], seq_len):\n",
        "        \n",
        "        # Grab feature characters\n",
        "        x = encoded_text[:, n:n+seq_len]\n",
        "        \n",
        "        # y is the target shifted over by 1\n",
        "        y = np.zeros_like(x)\n",
        "       \n",
        "        #\n",
        "        try:\n",
        "            y[:, :-1] = x[:, 1:]\n",
        "            y[:, -1]  = encoded_text[:, n+seq_len]\n",
        "            \n",
        "        # FOR POTENTIAL INDEXING ERROR AT THE END    \n",
        "        except:\n",
        "            y[:, :-1] = x[:, 1:]\n",
        "            y[:, -1] = encoded_text[:, 0]\n",
        "            \n",
        "        yield x, y"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAm6jJ2bfajJ"
      },
      "source": [
        "#example\n",
        "samplet = encoded_text[:50]\n",
        "sam = generate_batches(samplet,samp_per_batch=4,seq_len=12)\n",
        "x, y = next(sam)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KcD_Nuof1KQ",
        "outputId": "aab38e8e-7a5d-468d-f4a6-0a20a0d23075"
      },
      "source": [
        "x"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[16, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41],\n",
              "       [41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 26, 16],\n",
              "       [41, 41, 60, 62, 50, 70, 41, 10, 82, 20, 62, 33],\n",
              "       [39, 61, 41, 49, 62, 33, 82, 61, 81, 62, 33, 39]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXj3NNzMgGb9",
        "outputId": "5912d89f-3665-46fc-88aa-f55c8d2e3057"
      },
      "source": [
        "y"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 16],\n",
              "       [41, 41, 41, 41, 41, 41, 41, 41, 41, 26, 16, 41],\n",
              "       [41, 60, 62, 50, 70, 41, 10, 82, 20, 62, 33, 41],\n",
              "       [61, 41, 49, 62, 33, 82, 61, 81, 62, 33, 39, 39]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31sbCbL31FCL"
      },
      "source": [
        "Checking for availability of GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJiIk1vpgID4",
        "outputId": "a7dd926a-9c55-4c4c-dd46-94e74ed74778"
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hytrvNC_1LHh"
      },
      "source": [
        "Building LSTM model \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isqq4IXogT3j"
      },
      "source": [
        "class CharModel(nn.Module):\n",
        "    \n",
        "    def __init__(self, all_chars, num_hidden=256, num_layers=4,drop_prob=0.5,use_gpu=False):\n",
        "        \n",
        "        \n",
        "        # SET UP ATTRIBUTES\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "        self.num_layers = num_layers\n",
        "        self.num_hidden = num_hidden\n",
        "        self.use_gpu = use_gpu\n",
        "        \n",
        "        #CHARACTER SET, ENCODER, and DECODER\n",
        "        self.all_chars = all_chars\n",
        "        self.decoder = dict(enumerate(all_chars))\n",
        "        self.encoder = {char: ind for ind,char in decoder.items()}\n",
        "        \n",
        "        \n",
        "        self.lstm = nn.LSTM(len(self.all_chars), num_hidden, num_layers, dropout=drop_prob, batch_first=True)\n",
        "        \n",
        "        self.dropout = nn.Dropout(drop_prob)\n",
        "        \n",
        "        self.fc_linear = nn.Linear(num_hidden, len(self.all_chars))\n",
        "      \n",
        "    \n",
        "    def forward(self, x, hidden):\n",
        "                  \n",
        "        \n",
        "        lstm_output, hidden = self.lstm(x, hidden)\n",
        "        \n",
        "        \n",
        "        drop_output = self.dropout(lstm_output)\n",
        "        \n",
        "        drop_output = drop_output.contiguous().view(-1, self.num_hidden)\n",
        "        \n",
        "        \n",
        "        final_out = self.fc_linear(drop_output)\n",
        "        \n",
        "        \n",
        "        return final_out, hidden\n",
        "    \n",
        "    \n",
        "    def hidden_state(self, batch_size):\n",
        "        '''\n",
        "        Used as separate method to account for both GPU and CPU users.\n",
        "        '''\n",
        "        \n",
        "        if self.use_gpu:\n",
        "            \n",
        "            hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda(),\n",
        "                     torch.zeros(self.num_layers,batch_size,self.num_hidden).cuda())\n",
        "        else:\n",
        "            hidden = (torch.zeros(self.num_layers,batch_size,self.num_hidden),\n",
        "                     torch.zeros(self.num_layers,batch_size,self.num_hidden))\n",
        "        \n",
        "        return hidden\n",
        "        "
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zbMImg4oN5Hc"
      },
      "source": [
        "Instantiating The Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1yyHB18FgXw9"
      },
      "source": [
        "model = CharModel(\n",
        "    all_chars=all_characters,\n",
        "    num_hidden=512,\n",
        "    num_layers=3,\n",
        "    drop_prob=0.5,\n",
        "    use_gpu=True,\n",
        ")"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YLQLCUyOAld"
      },
      "source": [
        "Total Parameters in Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hq3Dj3eDgb3q"
      },
      "source": [
        "total_param  = []\n",
        "for p in model.parameters():\n",
        "    total_param.append(int(p.numel()))"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiy0uMCROLtH"
      },
      "source": [
        " **A good model have similar number of parameters as of the total characters in text**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZN47_9y2gfGT",
        "outputId": "04562c5f-6d8b-4f53-b424-d5e042b0ea5b"
      },
      "source": [
        "sum(total_param)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5470292"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syARVDVOgiiH",
        "outputId": "f1db3574-5159-472e-a431-c16ea35fe5b7"
      },
      "source": [
        "len(encoded_text)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5445609"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2hN5dFiOftG"
      },
      "source": [
        "setting optimizer and criterion for loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hqp0_-suglzZ"
      },
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JmOl3EhDgpU1"
      },
      "source": [
        "#training and val split\n",
        "train_percent = 0.1"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OxrFyelYg-Ge",
        "outputId": "55d5cd24-62aa-4fa2-efe7-dd9e3f37a446"
      },
      "source": [
        "int(len(encoded_text) * (train_percent))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "544560"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cE2bAXCrhC1J"
      },
      "source": [
        "train_ind = int(len(encoded_text) * (train_percent))"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bmwd6cnXhJCI"
      },
      "source": [
        "train_data = encoded_text[:train_ind]\n",
        "val_data = encoded_text[train_ind:]"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-v2mLlHPK_b"
      },
      "source": [
        "Training LSTM Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcFUI1zhhNUV"
      },
      "source": [
        "## VARIABLES\n",
        "\n",
        "# Epochs to train for\n",
        "epochs = 50\n",
        "# batch size \n",
        "batch_size = 128\n",
        "\n",
        "# Length of sequence\n",
        "seq_len = 100\n",
        "\n",
        "# for printing report purposes\n",
        "# always start at 0\n",
        "tracker = 0\n",
        "\n",
        "# number of characters in text\n",
        "num_char = max(encoded_text)+1"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmz7fGUahW16",
        "outputId": "e5cf05e4-203f-4874-eb28-5623b758aea4"
      },
      "source": [
        "# Set model to train\n",
        "model.train()\n",
        "\n",
        "\n",
        "# Check to see if using GPU\n",
        "if model.use_gpu:\n",
        "    model.cuda()\n",
        "\n",
        "for i in range(epochs):\n",
        "    \n",
        "    hidden = model.hidden_state(batch_size)\n",
        "    \n",
        "    \n",
        "    for x,y in generate_batches(train_data,batch_size,seq_len):\n",
        "        \n",
        "        tracker += 1\n",
        "        \n",
        "        # One Hot Encode incoming data\n",
        "        x = one_hot_encoder(x,num_char)\n",
        "        \n",
        "        # Convert Numpy Arrays to Tensor\n",
        "        \n",
        "        inputs = torch.from_numpy(x)\n",
        "        targets = torch.from_numpy(y)\n",
        "        \n",
        "        # Adjust for GPU if necessary\n",
        "        \n",
        "        if model.use_gpu:\n",
        "            \n",
        "            inputs = inputs.cuda()\n",
        "            targets = targets.cuda()\n",
        "            \n",
        "        # Reset Hidden State\n",
        "        # If we dont' reset we would backpropagate through all training history\n",
        "        hidden = tuple([state.data for state in hidden])\n",
        "        \n",
        "        model.zero_grad()\n",
        "        \n",
        "        lstm_output, hidden = model.forward(inputs,hidden)\n",
        "        loss = criterion(lstm_output,targets.view(batch_size*seq_len).long())\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        # POSSIBLE EXPLODING GRADIENT PROBLEM!\n",
        "        # LET\"S CLIP JUST IN CASE\n",
        "        nn.utils.clip_grad_norm_(model.parameters(),max_norm=5)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        \n",
        "        \n",
        "        ###################################\n",
        "        ### CHECK ON VALIDATION SET ######\n",
        "        #################################\n",
        "        \n",
        "        if tracker % 25 == 0:\n",
        "            \n",
        "            val_hidden = model.hidden_state(batch_size)\n",
        "            val_losses = []\n",
        "            model.eval()\n",
        "            \n",
        "            for x,y in generate_batches(val_data,batch_size,seq_len):\n",
        "                \n",
        "                # One Hot Encode incoming data\n",
        "                x = one_hot_encoder(x,num_char)\n",
        "                \n",
        "\n",
        "                # Convert Numpy Arrays to Tensor\n",
        "\n",
        "                inputs = torch.from_numpy(x)\n",
        "                targets = torch.from_numpy(y)\n",
        "\n",
        "                # Adjust for GPU if necessary\n",
        "\n",
        "                if model.use_gpu:\n",
        "\n",
        "                    inputs = inputs.cuda()\n",
        "                    targets = targets.cuda()\n",
        "                    \n",
        "                # Reset Hidden State\n",
        "                # If we dont' reset we would backpropagate through \n",
        "                # all training history\n",
        "                val_hidden = tuple([state.data for state in val_hidden])\n",
        "                \n",
        "                lstm_output, val_hidden = model.forward(inputs,val_hidden)\n",
        "                val_loss = criterion(lstm_output,targets.view(batch_size*seq_len).long())\n",
        "        \n",
        "                val_losses.append(val_loss.item())\n",
        "            \n",
        "            # Reset to training model after val for loop\n",
        "            model.train()\n",
        "            \n",
        "            print(f\"Epoch: {i} Step: {tracker} Val Loss: {val_loss.item()}\")"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 Step: 25 Val Loss: 3.2371671199798584\n",
            "Epoch: 1 Step: 50 Val Loss: 3.2342615127563477\n",
            "Epoch: 1 Step: 75 Val Loss: 3.233180284500122\n",
            "Epoch: 2 Step: 100 Val Loss: 3.1287336349487305\n",
            "Epoch: 2 Step: 125 Val Loss: 3.010040283203125\n",
            "Epoch: 3 Step: 150 Val Loss: 2.858440637588501\n",
            "Epoch: 4 Step: 175 Val Loss: 2.7408323287963867\n",
            "Epoch: 4 Step: 200 Val Loss: 2.6316757202148438\n",
            "Epoch: 5 Step: 225 Val Loss: 2.5085277557373047\n",
            "Epoch: 5 Step: 250 Val Loss: 2.4252593517303467\n",
            "Epoch: 6 Step: 275 Val Loss: 2.316955804824829\n",
            "Epoch: 7 Step: 300 Val Loss: 2.2501840591430664\n",
            "Epoch: 7 Step: 325 Val Loss: 2.198885202407837\n",
            "Epoch: 8 Step: 350 Val Loss: 2.152822971343994\n",
            "Epoch: 8 Step: 375 Val Loss: 2.117525339126587\n",
            "Epoch: 9 Step: 400 Val Loss: 2.078951835632324\n",
            "Epoch: 10 Step: 425 Val Loss: 2.0522267818450928\n",
            "Epoch: 10 Step: 450 Val Loss: 2.0250818729400635\n",
            "Epoch: 11 Step: 475 Val Loss: 2.0010972023010254\n",
            "Epoch: 11 Step: 500 Val Loss: 1.981205701828003\n",
            "Epoch: 12 Step: 525 Val Loss: 1.9615483283996582\n",
            "Epoch: 13 Step: 550 Val Loss: 1.9479440450668335\n",
            "Epoch: 13 Step: 575 Val Loss: 1.92414128780365\n",
            "Epoch: 14 Step: 600 Val Loss: 1.9114031791687012\n",
            "Epoch: 14 Step: 625 Val Loss: 1.890914797782898\n",
            "Epoch: 15 Step: 650 Val Loss: 1.8833178281784058\n",
            "Epoch: 16 Step: 675 Val Loss: 1.866959810256958\n",
            "Epoch: 16 Step: 700 Val Loss: 1.8535641431808472\n",
            "Epoch: 17 Step: 725 Val Loss: 1.853694200515747\n",
            "Epoch: 17 Step: 750 Val Loss: 1.830739140510559\n",
            "Epoch: 18 Step: 775 Val Loss: 1.8256033658981323\n",
            "Epoch: 19 Step: 800 Val Loss: 1.8166040182113647\n",
            "Epoch: 19 Step: 825 Val Loss: 1.8091732263565063\n",
            "Epoch: 20 Step: 850 Val Loss: 1.8035900592803955\n",
            "Epoch: 20 Step: 875 Val Loss: 1.7969356775283813\n",
            "Epoch: 21 Step: 900 Val Loss: 1.7961481809616089\n",
            "Epoch: 22 Step: 925 Val Loss: 1.7842391729354858\n",
            "Epoch: 22 Step: 950 Val Loss: 1.7813873291015625\n",
            "Epoch: 23 Step: 975 Val Loss: 1.7670756578445435\n",
            "Epoch: 23 Step: 1000 Val Loss: 1.7643672227859497\n",
            "Epoch: 24 Step: 1025 Val Loss: 1.75630521774292\n",
            "Epoch: 24 Step: 1050 Val Loss: 1.7566241025924683\n",
            "Epoch: 25 Step: 1075 Val Loss: 1.7697280645370483\n",
            "Epoch: 26 Step: 1100 Val Loss: 1.7436124086380005\n",
            "Epoch: 26 Step: 1125 Val Loss: 1.7461085319519043\n",
            "Epoch: 27 Step: 1150 Val Loss: 1.7429924011230469\n",
            "Epoch: 27 Step: 1175 Val Loss: 1.7407118082046509\n",
            "Epoch: 28 Step: 1200 Val Loss: 1.7346493005752563\n",
            "Epoch: 29 Step: 1225 Val Loss: 1.7320666313171387\n",
            "Epoch: 29 Step: 1250 Val Loss: 1.7327678203582764\n",
            "Epoch: 30 Step: 1275 Val Loss: 1.7291418313980103\n",
            "Epoch: 30 Step: 1300 Val Loss: 1.7391479015350342\n",
            "Epoch: 31 Step: 1325 Val Loss: 1.7254558801651\n",
            "Epoch: 32 Step: 1350 Val Loss: 1.7218257188796997\n",
            "Epoch: 32 Step: 1375 Val Loss: 1.7164862155914307\n",
            "Epoch: 33 Step: 1400 Val Loss: 1.721030592918396\n",
            "Epoch: 33 Step: 1425 Val Loss: 1.722580075263977\n",
            "Epoch: 34 Step: 1450 Val Loss: 1.718428611755371\n",
            "Epoch: 35 Step: 1475 Val Loss: 1.7313292026519775\n",
            "Epoch: 35 Step: 1500 Val Loss: 1.7186092138290405\n",
            "Epoch: 36 Step: 1525 Val Loss: 1.714247465133667\n",
            "Epoch: 36 Step: 1550 Val Loss: 1.719784140586853\n",
            "Epoch: 37 Step: 1575 Val Loss: 1.716956615447998\n",
            "Epoch: 38 Step: 1600 Val Loss: 1.7073155641555786\n",
            "Epoch: 38 Step: 1625 Val Loss: 1.7170060873031616\n",
            "Epoch: 39 Step: 1650 Val Loss: 1.7141127586364746\n",
            "Epoch: 39 Step: 1675 Val Loss: 1.725539207458496\n",
            "Epoch: 40 Step: 1700 Val Loss: 1.7166551351547241\n",
            "Epoch: 41 Step: 1725 Val Loss: 1.7139770984649658\n",
            "Epoch: 41 Step: 1750 Val Loss: 1.7195416688919067\n",
            "Epoch: 42 Step: 1775 Val Loss: 1.718011736869812\n",
            "Epoch: 42 Step: 1800 Val Loss: 1.7147995233535767\n",
            "Epoch: 43 Step: 1825 Val Loss: 1.720003366470337\n",
            "Epoch: 44 Step: 1850 Val Loss: 1.7177931070327759\n",
            "Epoch: 44 Step: 1875 Val Loss: 1.7185779809951782\n",
            "Epoch: 45 Step: 1900 Val Loss: 1.7202637195587158\n",
            "Epoch: 45 Step: 1925 Val Loss: 1.7257516384124756\n",
            "Epoch: 46 Step: 1950 Val Loss: 1.7254996299743652\n",
            "Epoch: 47 Step: 1975 Val Loss: 1.7254406213760376\n",
            "Epoch: 47 Step: 2000 Val Loss: 1.7339764833450317\n",
            "Epoch: 48 Step: 2025 Val Loss: 1.730281949043274\n",
            "Epoch: 48 Step: 2050 Val Loss: 1.7298648357391357\n",
            "Epoch: 49 Step: 2075 Val Loss: 1.73160719871521\n",
            "Epoch: 49 Step: 2100 Val Loss: 1.7361294031143188\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-5X5IINhXFi"
      },
      "source": [
        "model_name = 'shaks512_3'"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KjfsitwfhXXv"
      },
      "source": [
        "#Saving model \n",
        "torch.save(model.state_dict(),model_name)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uT6axUS-i2DR"
      },
      "source": [
        "# MUST MATCH THE EXACT SAME SETTINGS AS MODEL USED DURING TRAINING!\n",
        "#Loading Model\n",
        "model = CharModel(\n",
        "    all_chars=all_characters,\n",
        "    num_hidden=512,\n",
        "    num_layers=3,\n",
        "    drop_prob=0.5,\n",
        "    use_gpu=True,\n",
        ")"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aw9jihjjElR",
        "outputId": "0bf3646f-f964-448f-ec9d-24013ac959a0"
      },
      "source": [
        "model.load_state_dict(torch.load(model_name))\n",
        "model.eval()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CharModel(\n",
              "  (lstm): LSTM(84, 512, num_layers=3, batch_first=True, dropout=0.5)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (fc_linear): Linear(in_features=512, out_features=84, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r34gXMZ4jWVw"
      },
      "source": [
        "def predict_next_char(model, char, hidden=None, k=1):\n",
        "        \n",
        "        # Encode raw letters with model\n",
        "        encoded_text = model.encoder[char]\n",
        "        \n",
        "        # set as numpy array for one hot encoding\n",
        "        # NOTE THE [[ ]] dimensions!!\n",
        "        encoded_text = np.array([[encoded_text]])\n",
        "        \n",
        "        # One hot encoding\n",
        "        encoded_text = one_hot_encoder(encoded_text, len(model.all_chars))\n",
        "        \n",
        "        # Convert to Tensor\n",
        "        inputs = torch.from_numpy(encoded_text)\n",
        "        \n",
        "        # Check for CPU\n",
        "        if(model.use_gpu):\n",
        "            inputs = inputs.cuda()\n",
        "        \n",
        "        \n",
        "        # Grab hidden states\n",
        "        hidden = tuple([state.data for state in hidden])\n",
        "        \n",
        "        \n",
        "        # Run model and get predicted output\n",
        "        lstm_out, hidden = model(inputs, hidden)\n",
        "\n",
        "        \n",
        "        # Convert lstm_out to probabilities\n",
        "        probs = F.softmax(lstm_out, dim=1).data\n",
        "        \n",
        "        \n",
        "        \n",
        "        if(model.use_gpu):\n",
        "            # move back to CPU to use with numpy\n",
        "            probs = probs.cpu()\n",
        "        \n",
        "        \n",
        "        # k determines how many characters to consider\n",
        "        # for our probability choice.\n",
        "        # https://pytorch.org/docs/stable/torch.html#torch.topk\n",
        "        \n",
        "        # Return k largest probabilities in tensor\n",
        "        probs, index_positions = probs.topk(k)\n",
        "        \n",
        "        \n",
        "        index_positions = index_positions.numpy().squeeze()\n",
        "        \n",
        "        # Create array of probabilities\n",
        "        probs = probs.numpy().flatten()\n",
        "        \n",
        "        # Convert to probabilities per index\n",
        "        probs = probs/probs.sum()\n",
        "        \n",
        "        # randomly choose a character based on probabilities\n",
        "        char = np.random.choice(index_positions, p=probs)\n",
        "       \n",
        "        # return the encoded value of the predicted char and the hidden state\n",
        "        return model.decoder[char], hidden"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ueX3Bs6DjWHE"
      },
      "source": [
        "def generate_text(model, size, seed='The', k=1):\n",
        "        \n",
        "      \n",
        "    \n",
        "    # CHECK FOR GPU\n",
        "    if(model.use_gpu):\n",
        "        model.cuda()\n",
        "    else:\n",
        "        model.cpu()\n",
        "    \n",
        "    # Evaluation mode\n",
        "    model.eval()\n",
        "    \n",
        "    # begin output from initial seed\n",
        "    output_chars = [c for c in seed]\n",
        "    \n",
        "    # intiate hidden state\n",
        "    hidden = model.hidden_state(1)\n",
        "    \n",
        "    # predict the next character for every character in seed\n",
        "    for char in seed:\n",
        "        char, hidden = predict_next_char(model, char, hidden, k=k)\n",
        "    \n",
        "    # add initial characters to output\n",
        "    output_chars.append(char)\n",
        "    \n",
        "    # Now generate for size requested\n",
        "    for i in range(size):\n",
        "        \n",
        "        # predict based off very last letter in output_chars\n",
        "        char, hidden = predict_next_char(model, output_chars[-1], hidden, k=k)\n",
        "        \n",
        "        # add predicted character\n",
        "        output_chars.append(char)\n",
        "    \n",
        "    # return string of predicted text\n",
        "    return ''.join(output_chars)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H_-Z3nyPjV6l",
        "outputId": "0cd3176b-483e-4c5c-8b74-73d20aa803d1"
      },
      "source": [
        "print(generate_text(model, 1000, seed='CELIA ', k=3))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CELIA A. A mother of the wifler\n",
            "\n",
            "  AGRIPPA, and were a power of the sea,\n",
            "             And therefore have my sword,\n",
            "             As all to the wist strange,\n",
            "             The world will be the sea to stand,\n",
            "            That thou art to break this.\n",
            "  ANTONY. I'll be such a man see that I am sort,\n",
            "    And so will see his sword.\n",
            "  CLEOPATRA. I am all this word.\n",
            "  CLEOPATRA. The service warr'd to have an hearts of honour\n",
            "    A mother's tongue, which serve the wars\n",
            "    And to my sea of me, and so to be.\n",
            "    I have then this a servant, that I see\n",
            "    The world that we dispress. I was not to\n",
            "    Be sentery.                                  Exeunt\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "ACT III. SCENE 2.\n",
            "For the King and CLOWN\n",
            "\n",
            "  COUNTESS. Why, sir, the serve of him.\n",
            "  COUNTESS. Where is a man as the strong and this that hath the man's stole\n",
            "    as the sun one as I have not a pain to tell him.  \n",
            "  ORLANDO. What's the world is the world to them, and we shall see\n",
            "    to have to stard the forest of her stranger. I have heard thee\n",
            "    with t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LagKecAljVrk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}